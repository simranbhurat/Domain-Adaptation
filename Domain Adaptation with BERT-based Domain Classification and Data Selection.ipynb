{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c9a3e416-1828-49c4-8ffb-2d2777e3c62b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nIn the first phase, a domain classifier is trained on a pair of datasets.\\nThe goal is to teach the model how to distinguish between the two datasets\\nby learning their unique features and characteristics.\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "In the first phase, a domain classifier is trained on a pair of datasets.\n",
    "The goal is to teach the model how to distinguish between the two datasets\n",
    "by learning their unique features and characteristics.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64b4038f-fd86-4e97-ac5e-314f9097bb7d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import the required libraries\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "# Load the two datasets\n",
    "data1 = pd.read_csv('sample_data.csv', nrows = 4000)\n",
    "data1[\"label\"] = \"Patents\"\n",
    "data1['text'] = data1['patent_title'].astype(str) + ',' + data1['patent_abstract'].astype(str) + ',' + data1['summary_text'].astype(str)\n",
    "\n",
    "data2 = pd.read_csv('abstract_title_text_RD.csv', nrows = 4000)\n",
    "data2[\"label\"] = \"RD\"\n",
    "\n",
    "data1 = data1[[\"text\", \"label\"]].astype(str)\n",
    "data2 = data2[[\"text\", \"label\"]].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7a4d6c23-2bad-4364-a226-0ed195684ed8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 loss: 0.0021838047000346705\n",
      "0.99875\n"
     ]
    }
   ],
   "source": [
    "# Merge the datasets into a single dataframe\n",
    "data = pd.concat([data1, data2], ignore_index=True)\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "train_data, val_data = train_test_split(data, test_size=0.2)\n",
    "\n",
    "# Load the pre-trained BERT model and tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "\n",
    "# Set the device to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Tokenize the input data and convert to tensors\n",
    "def tokenize(data):\n",
    "    label_map = {label: i for i, label in enumerate(set(data['label'].to_list()))}\n",
    "    labels = [label_map[label] for label in data['label'].to_list()]\n",
    "    labels = torch.tensor(labels)\n",
    "    return tokenizer(data['text'].to_list(), padding=True, truncation=True, max_length=400, return_tensors='pt'), labels\n",
    "\n",
    "train_encodings, train_labels = tokenize(train_data)\n",
    "val_encodings, val_labels = tokenize(val_data)\n",
    "\n",
    "# Set up the optimizer and learning rate scheduler\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5, eps=1e-8)\n",
    "num_epochs = 1\n",
    "total_steps = len(train_data) * num_epochs\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(num_epochs):\n",
    "    # Set the model to training mode\n",
    "    model.train()\n",
    "\n",
    "    # Reset the loss for this epoch\n",
    "    total_loss = 0\n",
    "    total_mask = []\n",
    "    all_diffs = []\n",
    "    \n",
    "    # Train the model on batches of data\n",
    "    for i in range(0, len(train_data), 32):\n",
    "        # Clear gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Move the data to the device\n",
    "        batch_encodings = {key: val[i:i+32].to(device) for key, val in train_encodings.items()}\n",
    "        batch_labels = train_labels[i:i+32].to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(**batch_encodings, labels=batch_labels)\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "        # print(outputs.logits)\n",
    "        # apply softmax along the second dimension (classes)\n",
    "        probs = F.softmax(outputs.logits, dim=1)\n",
    "        # convert probabilities tensor to a NumPy array\n",
    "        probs_np = probs.detach().cpu().numpy()\n",
    "        diff = abs(probs_np[:, 1] - probs_np[:, 0]).tolist()\n",
    "        all_diffs.append(diff)\n",
    "\n",
    "        # # create boolean mask to select rows with probability between 0.5 and 0.7\n",
    "        # mask = np.logical_and(probs_np[:,0] > 0.4, probs_np[:,0] < 0.6)\n",
    "        # total_mask.append(mask)\n",
    "        # # select rows using boolean mask\n",
    "        # selected_rows = data1[mask]\n",
    "\n",
    "        # # print selected rows\n",
    "        # print(selected_rows)\n",
    "        # print(probs)\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        # Update parameters\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "    # Print the average loss for this epoch\n",
    "    print(f\"Epoch {epoch+1} loss: {total_loss/len(train_data)}\")\n",
    "\n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Evaluate the model on the validation set\n",
    "    with torch.no_grad():\n",
    "        val_loss = 0\n",
    "        total = 0\n",
    "        total_correct = 0\n",
    "        num_correct = 0\n",
    "        for i in range(0, len(val_data), 32):\n",
    "            # Move the data to the device\n",
    "            batch_encodings = {key: val[i:i+32].to(device) for key, val in val_encodings.items()}\n",
    "            batch_labels = val_labels[i:i+32].to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(**batch_encodings, labels=batch_labels)\n",
    "            loss = outputs.loss\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            # Calculate accuracy\n",
    "            logits = outputs.logits\n",
    "            predictions = torch.argmax(logits, dim=1)\n",
    "            total = total + len(predictions)\n",
    "            num_correct = torch.sum(predictions == batch_labels).item()\n",
    "            total_correct = total_correct + num_correct\n",
    "        print(total_correct/total)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "dc61aadd-0130-45b3-b082-c62cc2ed0c76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nIn the second phase, the source domain training samples are ranked based on the output from the domain classifier. \\nThis ranking process identifies which samples in the source domain are most similar to the target domain. \\nA subset of the top-ranked data points is then selected from the source domain training set.\\n'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "In the second phase, the source domain training samples are ranked based on the output from the domain classifier. \n",
    "This ranking process identifies which samples in the source domain are most similar to the target domain. \n",
    "A subset of the top-ranked data points is then selected from the source domain training set.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "af4473fb-7b45-4d99-8077-a481681d9d03",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.1351872980594635,\n",
       " 0.18749549984931946,\n",
       " 0.18018165230751038,\n",
       " 0.15372371673583984,\n",
       " 0.255646675825119,\n",
       " 0.1785520613193512,\n",
       " 0.19542551040649414,\n",
       " 0.021359652280807495,\n",
       " 0.26059383153915405,\n",
       " 0.14540478587150574,\n",
       " 0.25760963559150696,\n",
       " 0.20162546634674072,\n",
       " 0.1452009677886963,\n",
       " 0.007253885269165039,\n",
       " 0.14641380310058594,\n",
       " 0.23690757155418396,\n",
       " 0.26235729455947876,\n",
       " 0.2821299433708191,\n",
       " 0.11306524276733398,\n",
       " 0.23385867476463318,\n",
       " 0.1729470193386078,\n",
       " 0.09736809134483337,\n",
       " 0.1443699598312378,\n",
       " 0.11820340156555176,\n",
       " 0.16527783870697021,\n",
       " 0.19538092613220215,\n",
       " 0.0249767005443573,\n",
       " 0.047631144523620605,\n",
       " 0.16127610206604004,\n",
       " 0.09536507725715637,\n",
       " 0.10507968068122864,\n",
       " 0.1187359094619751,\n",
       " 0.1210012435913086,\n",
       " 0.46510830521583557,\n",
       " 0.4032003879547119,\n",
       " 0.2391432821750641,\n",
       " 0.3408139944076538,\n",
       " 0.2807919979095459,\n",
       " 0.3477397859096527,\n",
       " 0.06221318244934082,\n",
       " 0.008292943239212036,\n",
       " 0.051774442195892334,\n",
       " 0.005140513181686401,\n",
       " 0.38611742854118347,\n",
       " 0.25598832964897156,\n",
       " 0.3064034879207611,\n",
       " 0.3913826644420624,\n",
       " 0.3257046937942505,\n",
       " 0.09550800919532776,\n",
       " 0.3872906267642975,\n",
       " 0.21687528491020203,\n",
       " 0.02343866229057312,\n",
       " 0.05696451663970947,\n",
       " 0.3020626902580261,\n",
       " 0.0876399576663971,\n",
       " 0.34138962626457214,\n",
       " 0.02043938636779785,\n",
       " 0.47404590249061584,\n",
       " 0.1056152880191803,\n",
       " 0.17587095499038696,\n",
       " 0.22657126188278198,\n",
       " 0.15887507796287537,\n",
       " 0.37895330786705017,\n",
       " 0.518230676651001,\n",
       " 0.18755260109901428,\n",
       " 0.17839285731315613,\n",
       " 0.11384618282318115,\n",
       " 0.10322603583335876,\n",
       " 0.10116049647331238,\n",
       " 0.022115439176559448,\n",
       " 0.525653064250946,\n",
       " 0.3622925877571106,\n",
       " 0.007024854421615601,\n",
       " 0.2731441855430603,\n",
       " 0.14517542719841003,\n",
       " 0.18493670225143433,\n",
       " 0.20343342423439026,\n",
       " 0.3606799244880676,\n",
       " 0.13902655243873596,\n",
       " 0.41502076387405396,\n",
       " 0.15013229846954346,\n",
       " 0.07977819442749023,\n",
       " 0.0013958215713500977,\n",
       " 0.21954408288002014,\n",
       " 0.2060030698776245,\n",
       " 0.10590794682502747,\n",
       " 0.13731443881988525,\n",
       " 0.2365846037864685,\n",
       " 0.08758997917175293,\n",
       " 0.05086863040924072,\n",
       " 0.04111349582672119,\n",
       " 0.31237879395484924,\n",
       " 0.2485983669757843,\n",
       " 0.3045233190059662,\n",
       " 0.13040530681610107,\n",
       " 0.4198135733604431,\n",
       " 0.1050049364566803,\n",
       " 0.0019092559814453125,\n",
       " 0.3485657870769501,\n",
       " 0.13421982526779175,\n",
       " 0.10419213771820068,\n",
       " 0.1714543104171753,\n",
       " 0.12090608477592468,\n",
       " 0.08095169067382812,\n",
       " 0.23441284894943237,\n",
       " 0.14282646775245667,\n",
       " 0.1216106116771698,\n",
       " 0.3753395974636078,\n",
       " 0.33829283714294434,\n",
       " 0.2481374442577362,\n",
       " 0.03375852108001709,\n",
       " 0.013206720352172852,\n",
       " 0.21522071957588196,\n",
       " 0.40700867772102356,\n",
       " 0.0013527870178222656,\n",
       " 0.1866057813167572,\n",
       " 0.10203102231025696,\n",
       " 0.21866962313652039,\n",
       " 0.13814613223075867,\n",
       " 0.05449044704437256,\n",
       " 0.20605114102363586,\n",
       " 0.061297982931137085,\n",
       " 0.20818087458610535,\n",
       " 0.18153586983680725,\n",
       " 0.11238968372344971,\n",
       " 0.06001880764961243,\n",
       " 0.2581988275051117,\n",
       " 0.10889959335327148,\n",
       " 0.20558348298072815,\n",
       " 0.3434542119503021,\n",
       " 0.36459118127822876,\n",
       " 0.10903394222259521,\n",
       " 0.1543012261390686,\n",
       " 0.3075884282588959,\n",
       " 0.5013405084609985,\n",
       " 0.013019382953643799,\n",
       " 0.2729291319847107,\n",
       " 0.04866918921470642,\n",
       " 0.1985253095626831,\n",
       " 0.24986952543258667,\n",
       " 0.026853829622268677,\n",
       " 0.012349486351013184,\n",
       " 0.07396712899208069,\n",
       " 0.3001024127006531,\n",
       " 0.22504922747612,\n",
       " 0.20523947477340698,\n",
       " 0.11879685521125793,\n",
       " 0.039713531732559204,\n",
       " 0.3021540641784668,\n",
       " 0.35522255301475525,\n",
       " 0.17513525485992432,\n",
       " 0.4320429563522339,\n",
       " 0.10034897923469543,\n",
       " 0.23467707633972168,\n",
       " 0.1697084605693817,\n",
       " 0.16011464595794678,\n",
       " 0.2829990088939667,\n",
       " 0.22592473030090332,\n",
       " 0.2686297595500946,\n",
       " 0.15169072151184082,\n",
       " 0.08158424496650696,\n",
       " 0.30971378087997437,\n",
       " 0.10213211178779602,\n",
       " 0.01816597580909729,\n",
       " 0.09289312362670898,\n",
       " 0.2702217698097229,\n",
       " 0.11915948987007141,\n",
       " 0.3839671313762665,\n",
       " 0.22513318061828613,\n",
       " 0.009744614362716675,\n",
       " 0.0689736008644104,\n",
       " 0.17068731784820557,\n",
       " 0.22246649861335754,\n",
       " 0.0428658127784729,\n",
       " 0.30148717761039734,\n",
       " 0.0725189745426178,\n",
       " 0.039223432540893555,\n",
       " 0.20998823642730713,\n",
       " 0.14654123783111572,\n",
       " 0.2892657220363617,\n",
       " 0.06254571676254272,\n",
       " 0.04476842284202576,\n",
       " 0.1284991204738617,\n",
       " 0.25338730216026306,\n",
       " 0.4656296670436859,\n",
       " 0.2446615993976593,\n",
       " 0.12104365229606628,\n",
       " 0.189763605594635,\n",
       " 0.035179704427719116,\n",
       " 0.11857715249061584,\n",
       " 0.33165600895881653,\n",
       " 0.196041077375412,\n",
       " 0.15147554874420166,\n",
       " 0.23125645518302917,\n",
       " 0.1203920841217041,\n",
       " 0.22742879390716553,\n",
       " 0.26297879219055176,\n",
       " 0.1913316249847412,\n",
       " 0.26310253143310547,\n",
       " 0.07677769660949707,\n",
       " 0.35733166337013245,\n",
       " 0.33820319175720215,\n",
       " 0.3905310034751892,\n",
       " 0.2357388138771057,\n",
       " 0.18126586079597473,\n",
       " 0.23509278893470764,\n",
       " 0.08395394682884216,\n",
       " 0.33232972025871277,\n",
       " 0.18990251421928406,\n",
       " 0.23345637321472168,\n",
       " 0.1553943157196045,\n",
       " 0.3103850185871124,\n",
       " 0.19221064448356628,\n",
       " 0.1506795585155487,\n",
       " 0.2571296691894531,\n",
       " 0.3082374632358551,\n",
       " 0.036290377378463745,\n",
       " 0.12731516361236572,\n",
       " 0.37623634934425354,\n",
       " 0.26906251907348633,\n",
       " 0.2771759033203125,\n",
       " 0.3906247913837433,\n",
       " 0.1367516815662384,\n",
       " 0.21548673510551453,\n",
       " 0.25599753856658936,\n",
       " 0.0008134841918945312,\n",
       " 0.1612452268600464,\n",
       " 0.2274133563041687,\n",
       " 0.14005184173583984,\n",
       " 0.21930274367332458,\n",
       " 0.08596083521842957,\n",
       " 0.21695923805236816,\n",
       " 0.3319474160671234,\n",
       " 0.4077618420124054,\n",
       " 0.141490638256073,\n",
       " 0.3863604962825775,\n",
       " 0.13362360000610352,\n",
       " 0.06881004571914673,\n",
       " 0.26618099212646484,\n",
       " 0.1954871118068695,\n",
       " 0.017534852027893066,\n",
       " 0.07952934503555298,\n",
       " 0.06475484371185303,\n",
       " 0.33877670764923096,\n",
       " 0.4200259745121002,\n",
       " 0.323070764541626,\n",
       " 0.3342072367668152,\n",
       " 0.11528074741363525,\n",
       " 0.2734299898147583,\n",
       " 0.30183717608451843,\n",
       " 0.2005300521850586,\n",
       " 0.21647796034812927,\n",
       " 0.32843297719955444,\n",
       " 0.44386017322540283,\n",
       " 0.1828925907611847,\n",
       " 0.09310948848724365,\n",
       " 0.2344716489315033,\n",
       " 0.33389487862586975,\n",
       " 0.4058555066585541,\n",
       " 0.35352134704589844,\n",
       " 0.3246117830276489,\n",
       " 0.21806854009628296,\n",
       " 0.10178861021995544,\n",
       " 0.4185567796230316,\n",
       " 0.34462717175483704,\n",
       " 0.1624738872051239,\n",
       " 0.420667439699173,\n",
       " 0.3190927803516388,\n",
       " 0.2620990574359894,\n",
       " 0.31308722496032715,\n",
       " 0.2957670986652374,\n",
       " 0.3615388870239258,\n",
       " 0.18904277682304382,\n",
       " 0.1006619930267334,\n",
       " 0.4405529797077179,\n",
       " 0.4490296244621277,\n",
       " 0.08818462491035461,\n",
       " 0.21841371059417725,\n",
       " 0.4326535761356354,\n",
       " 0.4770491421222687,\n",
       " 0.4549849331378937,\n",
       " 0.004961967468261719,\n",
       " 0.30317482352256775,\n",
       " 0.29986727237701416,\n",
       " 0.45417213439941406,\n",
       " 0.11998328566551208,\n",
       " 0.44744494557380676,\n",
       " 0.30750593543052673,\n",
       " 0.36352068185806274,\n",
       " 0.23259246349334717,\n",
       " 0.0956263542175293,\n",
       " 0.33670735359191895,\n",
       " 0.07100507616996765,\n",
       " 0.11514711380004883,\n",
       " 0.4040931761264801,\n",
       " 0.37234610319137573,\n",
       " 0.2134152352809906,\n",
       " 0.09144482016563416,\n",
       " 0.34366723895072937,\n",
       " 0.16101336479187012,\n",
       " 0.29411613941192627,\n",
       " 0.3746231198310852,\n",
       " 0.3374740779399872,\n",
       " 0.4018784761428833,\n",
       " 0.25014111399650574,\n",
       " 0.3618098497390747,\n",
       " 0.4469855725765228,\n",
       " 0.1092439591884613,\n",
       " 0.3424433767795563,\n",
       " 0.4378966987133026,\n",
       " 0.5726668834686279,\n",
       " 0.22765734791755676,\n",
       " 0.49309664964675903,\n",
       " 0.4388878047466278,\n",
       " 0.3709413409233093,\n",
       " 0.25005412101745605,\n",
       " 0.5003342628479004,\n",
       " 0.4747205674648285,\n",
       " 0.16733548045158386,\n",
       " 0.39781510829925537,\n",
       " 0.1963864266872406,\n",
       " 0.3666648268699646,\n",
       " 0.470186710357666,\n",
       " 0.34564894437789917,\n",
       " 0.4978621006011963,\n",
       " 0.16327115893363953,\n",
       " 0.48171988129615784,\n",
       " 0.22273164987564087,\n",
       " 0.39022642374038696,\n",
       " 0.520944356918335,\n",
       " 0.21273663640022278,\n",
       " 0.2414584457874298,\n",
       " 0.2916821837425232,\n",
       " 0.43146297335624695,\n",
       " 0.4678272008895874,\n",
       " 0.20469248294830322,\n",
       " 0.3261260688304901,\n",
       " 0.1888054609298706,\n",
       " 0.22940832376480103,\n",
       " 0.4370420575141907,\n",
       " 0.25926369428634644,\n",
       " 0.16299280524253845,\n",
       " 0.3730435073375702,\n",
       " 0.4566371738910675,\n",
       " 0.4803568124771118,\n",
       " 0.29303979873657227,\n",
       " 0.4077165424823761,\n",
       " 0.12133097648620605,\n",
       " 0.22389835119247437,\n",
       " 0.4438978433609009,\n",
       " 0.25443029403686523,\n",
       " 0.22560352087020874,\n",
       " 0.11353632807731628,\n",
       " 0.5050458908081055,\n",
       " 0.5346639156341553,\n",
       " 0.330064982175827,\n",
       " 0.28461989760398865,\n",
       " 0.10561656951904297,\n",
       " 0.2804792821407318,\n",
       " 0.44354447722435,\n",
       " 0.15943700075149536,\n",
       " 0.49893033504486084,\n",
       " 0.4428615868091583,\n",
       " 0.21273663640022278,\n",
       " 0.5061764717102051,\n",
       " 0.4264805316925049,\n",
       " 0.5000520944595337,\n",
       " 0.3450224697589874,\n",
       " 0.5593973398208618,\n",
       " 0.3565150499343872,\n",
       " 0.12637114524841309,\n",
       " 0.3335826098918915,\n",
       " 0.4700862765312195,\n",
       " 0.48839545249938965,\n",
       " 0.48254334926605225,\n",
       " 0.27362897992134094,\n",
       " 0.21571886539459229,\n",
       " 0.05051624774932861,\n",
       " 0.53804612159729,\n",
       " 0.5119552612304688,\n",
       " 0.3543543219566345,\n",
       " 0.3860032856464386,\n",
       " 0.39991262555122375,\n",
       " 0.19886288046836853,\n",
       " 0.4710334241390228,\n",
       " 0.2831966280937195,\n",
       " 0.44001415371894836,\n",
       " 0.445326566696167,\n",
       " 0.4658966362476349,\n",
       " 0.4881906807422638,\n",
       " 0.43942129611968994,\n",
       " 0.4301221966743469,\n",
       " 0.5600899457931519,\n",
       " 0.27331027388572693,\n",
       " 0.5689126253128052,\n",
       " 0.06008577346801758,\n",
       " 0.4804631173610687,\n",
       " 0.5046453475952148,\n",
       " 0.5887507796287537,\n",
       " 0.45321720838546753,\n",
       " 0.3831760585308075,\n",
       " 0.48200523853302,\n",
       " 0.4994453191757202,\n",
       " 0.5951038002967834,\n",
       " 0.08911123871803284,\n",
       " 0.5511910915374756,\n",
       " 0.6050649881362915,\n",
       " 0.37262916564941406,\n",
       " 0.19869500398635864,\n",
       " 0.1048334538936615,\n",
       " 0.13754808902740479,\n",
       " 0.3305853307247162,\n",
       " 0.39088669419288635,\n",
       " 0.09710922837257385,\n",
       " 0.46276989579200745,\n",
       " 0.45398396253585815,\n",
       " 0.3698708415031433,\n",
       " 0.36791491508483887,\n",
       " 0.505095899105072,\n",
       " 0.5663245916366577,\n",
       " 0.27100875973701477,\n",
       " 0.5223696231842041,\n",
       " 0.4261845648288727,\n",
       " 0.5007100701332092,\n",
       " 0.37990477681159973,\n",
       " 0.16899996995925903,\n",
       " 0.1873234510421753,\n",
       " 0.3416781723499298,\n",
       " 0.47936952114105225,\n",
       " 0.5542675256729126,\n",
       " 0.5462043881416321,\n",
       " 0.5216660499572754,\n",
       " 0.18570783734321594,\n",
       " 0.5836749076843262,\n",
       " 0.5832891464233398,\n",
       " 0.2820422649383545,\n",
       " 0.1923583745956421,\n",
       " 0.518558144569397,\n",
       " 0.26840803027153015,\n",
       " 0.5210575461387634,\n",
       " 0.4898686408996582,\n",
       " 0.36828330159187317,\n",
       " 0.41158488392829895,\n",
       " 0.6558550000190735,\n",
       " 0.5253879427909851,\n",
       " 0.48993170261383057,\n",
       " 0.08460977673530579,\n",
       " 0.5817326307296753,\n",
       " 0.29067015647888184,\n",
       " 0.4123847484588623,\n",
       " 0.4671454131603241,\n",
       " 0.5950145721435547,\n",
       " 0.529868483543396,\n",
       " 0.3787388801574707,\n",
       " 0.5527641773223877,\n",
       " 0.4880921542644501,\n",
       " 0.4086510241031647,\n",
       " 0.3403331935405731,\n",
       " 0.5128779411315918,\n",
       " 0.6299852132797241,\n",
       " 0.5153244733810425,\n",
       " 0.4709189236164093,\n",
       " 0.5000720024108887,\n",
       " 0.6262378096580505,\n",
       " 0.4564226567745209,\n",
       " 0.3864305019378662,\n",
       " 0.5652401447296143,\n",
       " 0.5882702469825745,\n",
       " 0.2869846224784851,\n",
       " 0.690737247467041,\n",
       " 0.6293632984161377,\n",
       " 0.5181388854980469,\n",
       " 0.5374892950057983,\n",
       " 0.5745285749435425,\n",
       " 0.4777384102344513,\n",
       " 0.5925910472869873,\n",
       " 0.21623167395591736,\n",
       " 0.1934066116809845,\n",
       " 0.5631809234619141,\n",
       " 0.6109887957572937,\n",
       " 0.46034562587738037,\n",
       " 0.49750638008117676,\n",
       " 0.43852630257606506,\n",
       " 0.4850330650806427,\n",
       " 0.5292061567306519,\n",
       " 0.6167411804199219,\n",
       " 0.5150023102760315,\n",
       " 0.5257620811462402,\n",
       " 0.5070394277572632,\n",
       " 0.580424964427948,\n",
       " 0.6005499362945557,\n",
       " 0.6214085817337036,\n",
       " 0.6253952980041504,\n",
       " 0.48826655745506287,\n",
       " 0.34081628918647766,\n",
       " 0.08867520093917847,\n",
       " 0.22376754879951477,\n",
       " 0.5283875465393066,\n",
       " 0.3676445186138153,\n",
       " 0.27121177315711975,\n",
       " 0.43095463514328003,\n",
       " 0.545592188835144,\n",
       " 0.45681262016296387,\n",
       " 0.4247450828552246,\n",
       " 0.6557395458221436,\n",
       " 0.5232229232788086,\n",
       " 0.5376871228218079,\n",
       " 0.6422139406204224,\n",
       " 0.42095842957496643,\n",
       " 0.1988230049610138,\n",
       " 0.39654913544654846,\n",
       " 0.6045747995376587,\n",
       " 0.646174430847168,\n",
       " 0.45767560601234436,\n",
       " 0.214811772108078,\n",
       " 0.480126291513443,\n",
       " 0.47864261269569397,\n",
       " 0.5795581936836243,\n",
       " 0.281025230884552,\n",
       " 0.5731655359268188,\n",
       " 0.4256005585193634,\n",
       " 0.3101816475391388,\n",
       " 0.6548845171928406,\n",
       " 0.5172533988952637,\n",
       " 0.6331770420074463,\n",
       " 0.6502326130867004,\n",
       " 0.4195325970649719,\n",
       " 0.6210278272628784,\n",
       " 0.34381425380706787,\n",
       " 0.576055645942688,\n",
       " 0.5935916900634766,\n",
       " 0.4588795006275177,\n",
       " 0.24524250626564026,\n",
       " 0.6697330474853516,\n",
       " 0.6053630113601685,\n",
       " 0.4716472625732422,\n",
       " 0.46820369362831116,\n",
       " 0.6030722856521606,\n",
       " 0.05502426624298096,\n",
       " 0.5665197372436523,\n",
       " 0.4266399145126343,\n",
       " 0.4738765060901642,\n",
       " 0.5259705185890198,\n",
       " 0.642787516117096,\n",
       " 0.5663408041000366,\n",
       " 0.5701187252998352,\n",
       " 0.6250627040863037,\n",
       " 0.6386515498161316,\n",
       " 0.5848456621170044,\n",
       " 0.6085331439971924,\n",
       " 0.30235958099365234,\n",
       " 0.5362083911895752,\n",
       " 0.613939642906189,\n",
       " 0.6181386709213257,\n",
       " 0.5566893219947815,\n",
       " 0.4946296215057373,\n",
       " 0.5587074756622314,\n",
       " 0.6629899740219116,\n",
       " 0.44103074073791504,\n",
       " 0.5729092955589294,\n",
       " 0.6700485348701477,\n",
       " 0.021178722381591797,\n",
       " 0.4274907410144806,\n",
       " 0.5968179106712341,\n",
       " 0.6161720752716064,\n",
       " 0.4562113285064697,\n",
       " 0.46255287528038025,\n",
       " 0.6308046579360962,\n",
       " 0.6956574320793152,\n",
       " 0.3513430058956146,\n",
       " 0.699550449848175,\n",
       " 0.35467827320098877,\n",
       " 0.6042289733886719,\n",
       " 0.6395766735076904,\n",
       " 0.5858913660049438,\n",
       " 0.5504623055458069,\n",
       " 0.6219528317451477,\n",
       " 0.6639325618743896,\n",
       " 0.4540122151374817,\n",
       " 0.577652633190155,\n",
       " 0.6852527856826782,\n",
       " 0.6670531630516052,\n",
       " 0.6635100245475769,\n",
       " 0.6936382055282593,\n",
       " 0.6235287189483643,\n",
       " 0.6198850870132446,\n",
       " 0.6614608764648438,\n",
       " 0.6627224683761597,\n",
       " 0.682909369468689,\n",
       " 0.5914887189865112,\n",
       " 0.6262413263320923,\n",
       " 0.4119776487350464,\n",
       " 0.6070533990859985,\n",
       " 0.5798192024230957,\n",
       " 0.7305351495742798,\n",
       " 0.5150062441825867,\n",
       " 0.3422199487686157,\n",
       " 0.7243354320526123,\n",
       " 0.6600638628005981,\n",
       " 0.5135606527328491,\n",
       " 0.5470106601715088,\n",
       " 0.45405304431915283,\n",
       " 0.723832368850708,\n",
       " 0.32752978801727295,\n",
       " 0.7176569700241089,\n",
       " 0.7425873279571533,\n",
       " 0.5745534896850586,\n",
       " 0.638137698173523,\n",
       " 0.6904608607292175,\n",
       " 0.7928313612937927,\n",
       " 0.6705496311187744,\n",
       " 0.6472843289375305,\n",
       " 0.4965386390686035,\n",
       " 0.45436158776283264,\n",
       " 0.09041157364845276,\n",
       " 0.6315119862556458,\n",
       " 0.5218760371208191,\n",
       " 0.5035783648490906,\n",
       " 0.5522474646568298,\n",
       " 0.494118332862854,\n",
       " 0.3674454689025879,\n",
       " 0.7216020822525024,\n",
       " 0.13162922859191895,\n",
       " 0.7144527435302734,\n",
       " 0.1298796832561493,\n",
       " 0.6217547059059143,\n",
       " 0.6136410236358643,\n",
       " 0.6590867638587952,\n",
       " 0.6385481357574463,\n",
       " 0.33937957882881165,\n",
       " 0.6535602807998657,\n",
       " 0.7461631298065186,\n",
       " 0.6905585527420044,\n",
       " 0.6748107671737671,\n",
       " 0.531440258026123,\n",
       " 0.7334296703338623,\n",
       " 0.6573445200920105,\n",
       " 0.7435665130615234,\n",
       " 0.6108429431915283,\n",
       " 0.7281827926635742,\n",
       " 0.6523510217666626,\n",
       " 0.7029684782028198,\n",
       " 0.6753026247024536,\n",
       " 0.7544538974761963,\n",
       " 0.57197105884552,\n",
       " 0.41255518794059753,\n",
       " 0.747407853603363,\n",
       " 0.40850913524627686,\n",
       " 0.688821017742157,\n",
       " 0.7642988562583923,\n",
       " 0.413077712059021,\n",
       " 0.609137773513794,\n",
       " 0.6290606260299683,\n",
       " 0.7652615904808044,\n",
       " 0.6879066228866577,\n",
       " 0.79961758852005,\n",
       " 0.6453955173492432,\n",
       " 0.32758867740631104,\n",
       " 0.042098015546798706,\n",
       " 0.6499713659286499,\n",
       " 0.5234886407852173,\n",
       " 0.6497570276260376,\n",
       " 0.4625976085662842,\n",
       " 0.33635804057121277,\n",
       " 0.7353124022483826,\n",
       " 0.7902241349220276,\n",
       " 0.72569739818573,\n",
       " 0.6763607263565063,\n",
       " 0.6023366451263428,\n",
       " 0.4507788121700287,\n",
       " 0.6809198260307312,\n",
       " 0.1963633894920349,\n",
       " 0.6340837478637695,\n",
       " 0.6267653107643127,\n",
       " 0.5734299421310425,\n",
       " 0.03462103009223938,\n",
       " 0.6312854886054993,\n",
       " 0.7410988211631775,\n",
       " 0.6128330826759338,\n",
       " 0.5783727169036865,\n",
       " 0.1951635777950287,\n",
       " 0.704826831817627,\n",
       " 0.602002739906311,\n",
       " 0.4645178020000458,\n",
       " 0.6624889373779297,\n",
       " 0.7806079983711243,\n",
       " 0.6430708765983582,\n",
       " 0.6418586373329163,\n",
       " 0.6569358110427856,\n",
       " 0.7155426144599915,\n",
       " 0.7643042802810669,\n",
       " 0.5601641535758972,\n",
       " 0.6966725587844849,\n",
       " 0.6533209085464478,\n",
       " 0.7590421438217163,\n",
       " 0.7971601486206055,\n",
       " 0.6724333763122559,\n",
       " 0.6994349956512451,\n",
       " 0.6559635400772095,\n",
       " 0.636504590511322,\n",
       " 0.7907232046127319,\n",
       " 0.52161705493927,\n",
       " 0.7463351488113403,\n",
       " 0.6940234899520874,\n",
       " 0.6393436193466187,\n",
       " 0.6533706188201904,\n",
       " 0.743253231048584,\n",
       " 0.6297001838684082,\n",
       " 0.6564025282859802,\n",
       " 0.7637172937393188,\n",
       " 0.6663391590118408,\n",
       " 0.7399738430976868,\n",
       " 0.7666332721710205,\n",
       " 0.760043740272522,\n",
       " 0.7865477800369263,\n",
       " 0.7739527225494385,\n",
       " 0.7986971735954285,\n",
       " 0.6384358406066895,\n",
       " 0.7946045398712158,\n",
       " 0.7419309616088867,\n",
       " 0.724603533744812,\n",
       " 0.7192596197128296,\n",
       " 0.7664062976837158,\n",
       " 0.8318932056427002,\n",
       " 0.691468358039856,\n",
       " 0.7823901176452637,\n",
       " 0.77984619140625,\n",
       " 0.5848739743232727,\n",
       " 0.5955681800842285,\n",
       " 0.6765347719192505,\n",
       " 0.823566734790802,\n",
       " 0.36711743474006653,\n",
       " 0.7572809457778931,\n",
       " 0.8177153468132019,\n",
       " 0.7376723289489746,\n",
       " 0.6993380784988403,\n",
       " 0.7391683459281921,\n",
       " 0.6776350736618042,\n",
       " 0.7602069973945618,\n",
       " 0.6322692632675171,\n",
       " 0.8315399885177612,\n",
       " 0.6054819822311401,\n",
       " 0.48351287841796875,\n",
       " 0.5737226009368896,\n",
       " 0.7625607252120972,\n",
       " 0.6962586045265198,\n",
       " 0.7555228471755981,\n",
       " 0.5517796277999878,\n",
       " 0.7611140012741089,\n",
       " 0.6348878145217896,\n",
       " 0.7250616550445557,\n",
       " 0.673748254776001,\n",
       " 0.7370599508285522,\n",
       " 0.687252402305603,\n",
       " 0.7602158188819885,\n",
       " 0.769707441329956,\n",
       " 0.7187007665634155,\n",
       " 0.8117817044258118,\n",
       " 0.6953960657119751,\n",
       " 0.7338694930076599,\n",
       " 0.7539384365081787,\n",
       " 0.6493845582008362,\n",
       " 0.6887863278388977,\n",
       " 0.6956095695495605,\n",
       " 0.7825136780738831,\n",
       " 0.6868860721588135,\n",
       " 0.774938702583313,\n",
       " 0.7612752914428711,\n",
       " 0.7725076675415039,\n",
       " 0.8005280494689941,\n",
       " 0.7044020891189575,\n",
       " 0.7434532642364502,\n",
       " 0.7740954756736755,\n",
       " 0.7765567302703857,\n",
       " 0.775999128818512,\n",
       " 0.6992784738540649,\n",
       " 0.8339942693710327,\n",
       " 0.8169857859611511,\n",
       " 0.7372739315032959,\n",
       " 0.817886471748352,\n",
       " 0.5522609949111938,\n",
       " 0.5217645168304443,\n",
       " 0.8438524603843689,\n",
       " 0.7270540595054626,\n",
       " 0.790706992149353,\n",
       " 0.6695327162742615,\n",
       " 0.7473887801170349,\n",
       " 0.7854332327842712,\n",
       " 0.8286067247390747,\n",
       " 0.7865411043167114,\n",
       " 0.8314878344535828,\n",
       " 0.7544617056846619,\n",
       " 0.776750922203064,\n",
       " 0.19167399406433105,\n",
       " 0.7526776790618896,\n",
       " 0.6927049160003662,\n",
       " 0.7969399690628052,\n",
       " 0.7726150155067444,\n",
       " 0.6205176115036011,\n",
       " 0.6586054563522339,\n",
       " 0.8105035424232483,\n",
       " 0.7367342710494995,\n",
       " 0.393642395734787,\n",
       " 0.8120152950286865,\n",
       " 0.7349157929420471,\n",
       " 0.8283755779266357,\n",
       " 0.8008941411972046,\n",
       " 0.3027767241001129,\n",
       " 0.6902768611907959,\n",
       " 0.8135695457458496,\n",
       " 0.8229370713233948,\n",
       " 0.6261699199676514,\n",
       " 0.8053793907165527,\n",
       " 0.7500143051147461,\n",
       " 0.6531317830085754,\n",
       " 0.8536548018455505,\n",
       " 0.735552966594696,\n",
       " 0.7634413838386536,\n",
       " 0.7289659380912781,\n",
       " 0.8286046981811523,\n",
       " 0.7986595630645752,\n",
       " 0.7757395505905151,\n",
       " 0.7192196249961853,\n",
       " 0.7381883263587952,\n",
       " 0.8073046803474426,\n",
       " 0.6847231388092041,\n",
       " 0.8498103022575378,\n",
       " 0.7614753842353821,\n",
       " 0.7663708329200745,\n",
       " 0.7755122184753418,\n",
       " 0.8229036927223206,\n",
       " 0.791469156742096,\n",
       " 0.861295223236084,\n",
       " 0.8468884825706482,\n",
       " 0.8636671304702759,\n",
       " 0.8542535901069641,\n",
       " 0.708416223526001,\n",
       " 0.7274244427680969,\n",
       " 0.6734256148338318,\n",
       " 0.7987881898880005,\n",
       " 0.8187038898468018,\n",
       " 0.8145463466644287,\n",
       " 0.8460989594459534,\n",
       " 0.7813997268676758,\n",
       " 0.8059972524642944,\n",
       " 0.7358825206756592,\n",
       " 0.7073007822036743,\n",
       " 0.8729826807975769,\n",
       " 0.6922367811203003,\n",
       " 0.885686993598938,\n",
       " 0.7579761743545532,\n",
       " 0.7893812656402588,\n",
       " 0.5400643348693848,\n",
       " 0.858109712600708,\n",
       " 0.8125023245811462,\n",
       " 0.8579084873199463,\n",
       " 0.7712490558624268,\n",
       " 0.7238197922706604,\n",
       " 0.8643824458122253,\n",
       " 0.8313424587249756,\n",
       " 0.8877788186073303,\n",
       " 0.8694009780883789,\n",
       " 0.8546494245529175,\n",
       " 0.8195888996124268,\n",
       " 0.8637983798980713,\n",
       " 0.8183138370513916,\n",
       " 0.7896584272384644,\n",
       " 0.7551937103271484,\n",
       " 0.8236746788024902,\n",
       " 0.7941842079162598,\n",
       " 0.8893855214118958,\n",
       " 0.8086043000221252,\n",
       " 0.8574278950691223,\n",
       " 0.2652449309825897,\n",
       " 0.6993994116783142,\n",
       " 0.8356570601463318,\n",
       " 0.8806588053703308,\n",
       " 0.8097362518310547,\n",
       " 0.7734848260879517,\n",
       " 0.8436062932014465,\n",
       " 0.8076636791229248,\n",
       " 0.8051414489746094,\n",
       " 0.8188881874084473,\n",
       " 0.8184614777565002,\n",
       " 0.8598113059997559,\n",
       " 0.7664457559585571,\n",
       " 0.7781838178634644,\n",
       " 0.8405681252479553,\n",
       " 0.8478387594223022,\n",
       " 0.8105717897415161,\n",
       " 0.859146237373352,\n",
       " 0.8701651096343994,\n",
       " 0.8771040439605713,\n",
       " 0.8097184300422668,\n",
       " 0.7905943989753723,\n",
       " 0.8413110971450806,\n",
       " 0.870299220085144,\n",
       " 0.8824971914291382,\n",
       " 0.7122056484222412,\n",
       " 0.6372735500335693,\n",
       " 0.8350365161895752,\n",
       " 0.764200747013092,\n",
       " 0.9046852588653564,\n",
       " 0.8701049089431763,\n",
       " 0.7694916725158691,\n",
       " 0.7505508661270142,\n",
       " 0.8096743822097778,\n",
       " 0.8366118669509888,\n",
       " 0.8393893837928772,\n",
       " 0.7290011644363403,\n",
       " 0.8605060577392578,\n",
       " 0.857884407043457,\n",
       " 0.6799221038818359,\n",
       " 0.8506502509117126,\n",
       " 0.8822393417358398,\n",
       " 0.8110161423683167,\n",
       " 0.8118447065353394,\n",
       " 0.7958427667617798,\n",
       " 0.8089916706085205,\n",
       " 0.5770746469497681,\n",
       " 0.9034361839294434,\n",
       " 0.8102195858955383,\n",
       " 0.865863561630249,\n",
       " 0.8845845460891724,\n",
       " 0.8128860592842102,\n",
       " 0.8354860544204712,\n",
       " 0.8439925312995911,\n",
       " 0.9225422739982605,\n",
       " 0.6932095885276794,\n",
       " 0.8553627133369446,\n",
       " 0.861693799495697,\n",
       " 0.7501010894775391,\n",
       " 0.7679926753044128,\n",
       " 0.8412132859230042,\n",
       " 0.868008553981781,\n",
       " 0.8126697540283203,\n",
       " 0.8241945505142212,\n",
       " 0.7805541753768921,\n",
       " 0.8626387715339661,\n",
       " 0.9019702076911926,\n",
       " 0.855466365814209,\n",
       " 0.7612852454185486,\n",
       " 0.8653475642204285,\n",
       " 0.8442538380622864,\n",
       " 0.8281912803649902,\n",
       " 0.7919484376907349,\n",
       " 0.8756163716316223,\n",
       " 0.8130617737770081,\n",
       " 0.8755321502685547,\n",
       " 0.84507817029953,\n",
       " 0.8800739645957947,\n",
       " 0.9034349322319031,\n",
       " 0.8801358938217163,\n",
       " 0.8259860277175903,\n",
       " 0.8474820852279663,\n",
       " 0.880975067615509,\n",
       " 0.8716193437576294,\n",
       " 0.872377336025238,\n",
       " 0.9132034778594971,\n",
       " 0.8346205949783325,\n",
       " 0.8021988868713379,\n",
       " 0.803283154964447,\n",
       " 0.8474341034889221,\n",
       " 0.9207696914672852,\n",
       " 0.9250205159187317,\n",
       " 0.883392333984375,\n",
       " 0.8905799388885498,\n",
       " 0.25803983211517334,\n",
       " 0.935184121131897,\n",
       " 0.9298579692840576,\n",
       " 0.8109796643257141,\n",
       " 0.8744992017745972,\n",
       " 0.8694849610328674,\n",
       " 0.7688329219818115,\n",
       " 0.8832179307937622,\n",
       " 0.7844176888465881,\n",
       " 0.8619702458381653,\n",
       " 0.9054133892059326,\n",
       " 0.8585114479064941,\n",
       " 0.8891834616661072,\n",
       " 0.8830621838569641,\n",
       " 0.8658157587051392,\n",
       " 0.6691274046897888,\n",
       " 0.9095854163169861,\n",
       " 0.8240904808044434,\n",
       " 0.8262433409690857,\n",
       " 0.8124784231185913,\n",
       " 0.4830886125564575,\n",
       " 0.7685133218765259,\n",
       " 0.7761469483375549,\n",
       " 0.8828656673431396,\n",
       " 0.8922122120857239,\n",
       " 0.8966873288154602,\n",
       " 0.8930691480636597,\n",
       " 0.9006560444831848,\n",
       " 0.9190429449081421,\n",
       " 0.9050928950309753,\n",
       " 0.8891754746437073,\n",
       " 0.9003713726997375,\n",
       " 0.7496192455291748,\n",
       " ...]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import itertools\n",
    "flattened = list(itertools.chain.from_iterable(all_diffs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c1a0367f-c0eb-4e46-8e61-11b98dd88bac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_data['diff'] = flattened"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "4fd9b36a-b431-4240-bef9-b12f77f11dc0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5934</td>\n",
       "      <td>Bichromates of organic amines and onium salts ...</td>\n",
       "      <td>RD</td>\n",
       "      <td>0.000813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4883</td>\n",
       "      <td>Diazotype compositions and photographic elemen...</td>\n",
       "      <td>RD</td>\n",
       "      <td>0.001396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4735</td>\n",
       "      <td>Photosensitive silver halide materials \\n\\n902...</td>\n",
       "      <td>RD</td>\n",
       "      <td>0.004962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5115</td>\n",
       "      <td>274002\\n\\nA method for synchronizing two clock...</td>\n",
       "      <td>RD</td>\n",
       "      <td>0.005141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4568</td>\n",
       "      <td>Pseudo-end-capped Polyamic-acids \\n\\n8535\\n\\nN...</td>\n",
       "      <td>RD</td>\n",
       "      <td>0.008293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3217</th>\n",
       "      <td>7542</td>\n",
       "      <td>14447\\n\\nPositive-working immobile photographi...</td>\n",
       "      <td>RD</td>\n",
       "      <td>0.999324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3218</th>\n",
       "      <td>7034</td>\n",
       "      <td>296023\\n\\nConstruction to Reduce Stress in TCM...</td>\n",
       "      <td>RD</td>\n",
       "      <td>0.999333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3219</th>\n",
       "      <td>6345</td>\n",
       "      <td>Improvements in the analyses of metals \\n\\n124...</td>\n",
       "      <td>RD</td>\n",
       "      <td>0.999339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3220</th>\n",
       "      <td>6927</td>\n",
       "      <td>295032\\n\\nStepper Motor Phase Protection Circu...</td>\n",
       "      <td>RD</td>\n",
       "      <td>0.999380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3221</th>\n",
       "      <td>7910</td>\n",
       "      <td>304079\\n\\nApparatus to Increase Magnetic Field...</td>\n",
       "      <td>RD</td>\n",
       "      <td>0.999393</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3222 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      index                                               text label      diff\n",
       "0      5934  Bichromates of organic amines and onium salts ...    RD  0.000813\n",
       "1      4883  Diazotype compositions and photographic elemen...    RD  0.001396\n",
       "2      4735  Photosensitive silver halide materials \\n\\n902...    RD  0.004962\n",
       "3      5115  274002\\n\\nA method for synchronizing two clock...    RD  0.005141\n",
       "4      4568  Pseudo-end-capped Polyamic-acids \\n\\n8535\\n\\nN...    RD  0.008293\n",
       "...     ...                                                ...   ...       ...\n",
       "3217   7542  14447\\n\\nPositive-working immobile photographi...    RD  0.999324\n",
       "3218   7034  296023\\n\\nConstruction to Reduce Stress in TCM...    RD  0.999333\n",
       "3219   6345  Improvements in the analyses of metals \\n\\n124...    RD  0.999339\n",
       "3220   6927  295032\\n\\nStepper Motor Phase Protection Circu...    RD  0.999380\n",
       "3221   7910  304079\\n\\nApparatus to Increase Magnetic Field...    RD  0.999393\n",
       "\n",
       "[3222 rows x 4 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# sort the dataframe by the 'diff' column in ascending order\n",
    "train_data_sorted = train_data.sort_values('diff')\n",
    "\n",
    "# show only the rows where the 'label' column is 'RD'\n",
    "rd_rows = train_data_sorted[train_data_sorted['label'] == 'RD']\n",
    "rd_rows.reset_index(inplace = True)\n",
    "# print the resulting dataframe\n",
    "display(rd_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330096dd-843e-4f9a-a7c9-75e3cb5bb7cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
